# encoding: utf-8
import feedparser
import requests
import json
import datetime
import time
import pytz
from time import mktime
from config import *

# === è®¾ç½®å›é¡¾æ—¶é—´èŒƒå›´ ===
# ä¿®æ”¹ä¸º 30 å¤© (å³ä¸€ä¸ªæœˆ)
MAX_LOOKBACK_DAYS = 30 

# === å»é‡æ£€æµ‹èŒƒå›´ ===
# ä¸ºäº†é…åˆ 30 å¤©çš„æ—¶é—´çª—å£ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ›´å¤šçš„å†å² Issue
# å»ºè®®è®¾ä¸º 45 æˆ– 60ï¼Œç¡®ä¿èƒ½è¦†ç›–è¿‡å»ä¸€ä¸ªå¤šæœˆçš„è®°å½•
DUPLICATE_CHECK_COUNT = 45

def get_current_date():
    tz = pytz.timezone('Asia/Shanghai')
    return datetime.datetime.now(tz).strftime('%Y-%m-%d')

def is_recent_paper(entry):
    """åˆ¤æ–­è®ºæ–‡æ˜¯å¦åœ¨æœ€è¿‘ MAX_LOOKBACK_DAYS å¤©å†…å‘å¸ƒ"""
    try:
        published_struct = getattr(entry, 'published_parsed', None) or getattr(entry, 'updated_parsed', None)
        if not published_struct:
            return True
        pub_date = datetime.datetime.fromtimestamp(mktime(published_struct))
        current_date = datetime.datetime.now()
        delta = current_date - pub_date
        return delta.days <= MAX_LOOKBACK_DAYS
    except Exception as e:
        return True 

def get_already_sent_links():
    """
    è·å–æœ€è¿‘å‘å¸ƒçš„ Issue å†…å®¹ï¼Œæå–å‡ºæ‰€æœ‰å·²å‘é€è¿‡çš„é“¾æ¥ã€‚
    ç”¨äºå»é‡ã€‚
    """
    if not TOKEN:
        print("è­¦å‘Šï¼šæœªè®¾ç½® Tokenï¼Œæ— æ³•è·å–å†å²è®°å½•è¿›è¡Œå»é‡ã€‚")
        return set()

    print(f"æ­£åœ¨æ£€æŸ¥å†å² Issue ä»¥å»é‡ (æ£€æŸ¥æœ€è¿‘ {DUPLICATE_CHECK_COUNT} ä¸ª)...")
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    params = {
        "state": "all",          # å³ä½¿å…³é—­çš„ Issue ä¹Ÿè¦æ£€æŸ¥
        "labels": "daily-report", # åªæ£€æŸ¥æˆ‘ä»¬æœºå™¨äººå‘çš„
        "per_page": DUPLICATE_CHECK_COUNT
    }
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code != 200:
            print(f"è·å–å†å²è®°å½•å¤±è´¥: {response.status_code}")
            return set()
        
        issues = response.json()
        sent_links = set()
        
        # éå†å†å² Issue çš„å†…å®¹
        for issue in issues:
            body = issue.get('body', '')
            if body:
                # è®°å½•æ•´ä¸ª Issue å†…å®¹ç”¨äºæŸ¥é‡
                sent_links.add(body) 
        
        print(f"âœ… å·²åŠ è½½å†å²è®°å½•ï¼Œå‡†å¤‡è¿‡æ»¤...")
        return sent_links
        
    except Exception as e:
        print(f"è·å–å†å²è®°å½•å‡ºé”™: {e}")
        return set()

def fetch_rss_papers():
    # 1. å…ˆè·å–å†å²è®°å½•
    history_contents = get_already_sent_links()
    
    print(f"å¼€å§‹æŠ“å–ä»»åŠ¡... (åªçœ‹æœ€è¿‘ {MAX_LOOKBACK_DAYS} å¤©)")
    found_papers = []
    
    for source in RSS_FEEDS:
        print(f"æ­£åœ¨æ£€æŸ¥: {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                continue

            for entry in feed.entries:
                # --- æ—¶é—´è¿‡æ»¤å™¨ ---
                if not is_recent_paper(entry):
                    continue 

                title = entry.title
                summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                published = getattr(entry, 'published', '') or getattr(entry, 'updated', 'Unknown Date')
                
                # --- å»é‡é€»è¾‘ ---
                is_duplicate = False
                for old_body in history_contents:
                    if link in old_body:
                        is_duplicate = True
                        break
                
                if is_duplicate:
                    continue
                # ----------------

                content_to_check = (title + summary).lower()
                
                matched_keywords = []
                for kw in KEYWORD_LIST:
                    if kw.lower() in content_to_check:
                        matched_keywords.append(kw)
                
                if matched_keywords:
                    paper_info = {
                        'source': source['name'],
                        'title': title.replace('\n', ' '),
                        'link': link,
                        'date': published,
                        'keywords': matched_keywords
                    }
                    found_papers.append(paper_info)
                    
        except Exception as e:
            print(f"æŠ“å– {source['name']} å¤±è´¥: {e}")
            
    return found_papers

def generate_markdown(papers):
    if not papers:
        return None
    
    date_str = get_current_date()
    md_content = f"# ğŸ“… Daily Paper Update: {date_str}\n\n"
    md_content += f"**ä»Šæ—¥å‘ç° {len(papers)} ç¯‡è¿‘æœŸ({MAX_LOOKBACK_DAYS}å¤©å†…)ç›¸å…³è®ºæ–‡**\n\n---"
    
    current_source = ""
    for paper in papers:
        if paper['source'] != current_source:
            current_source = paper['source']
            md_content += f"\n\n## ğŸ“š {current_source}\n"
        
        kw_str = ", ".join([f"`{k}`" for k in paper['keywords']])
        md_content += f"\n### [{paper['title']}]({paper['link']})\n"
        md_content += f"- **å…³é”®è¯**: {kw_str}\n"
        md_content += f"- **å‘å¸ƒæ—¶é—´**: {paper['date']}\n"
        
    return md_content

def post_github_issue(content):
    if not content:
        print("ä»Šæ—¥æ— æ–°å‘ç°ï¼ˆæˆ–å…¨éƒ¨å·²å»é‡ï¼‰ï¼Œä¸åˆ›å»º Issueã€‚")
        return

    if not TOKEN:
        print("é”™è¯¯ï¼šæœªè®¾ç½® GH_TOKENï¼Œæ— æ³•å‘é€ Issueã€‚")
        return

    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    date_str = get_current_date()
    payload = {
        "title": f"[{date_str}] Daily Papers ({len(content.split('###')) - 1} papers)",
        "body": content,
        "labels": ["daily-report"]
    }
    
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    
    if response.status_code == 201:
        print("âœ… Issue åˆ›å»ºæˆåŠŸï¼")
    else:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
        print(response.text)

if __name__ == '__main__':
    papers = fetch_rss_papers()
    md_text = generate_markdown(papers)
    post_github_issue(md_text)
