# encoding: utf-8
import feedparser
import requests
import json
import datetime
import time
import pytz
from time import mktime
from config import *

# === è®¾ç½®å›é¡¾æ—¶é—´èŒƒå›´ ===
# åªæŠ“å–è¿‡å» 14 å¤©å†…çš„è®ºæ–‡ï¼Œé¿å…æŠ“åˆ°è€æ—§æ•°æ®
MAX_LOOKBACK_DAYS = 14 

def get_current_date():
    tz = pytz.timezone('Asia/Shanghai')
    return datetime.datetime.now(tz).strftime('%Y-%m-%d')

def is_recent_paper(entry):
    """
    åˆ¤æ–­è®ºæ–‡æ˜¯å¦åœ¨æœ€è¿‘ MAX_LOOKBACK_DAYS å¤©å†…å‘å¸ƒ
    """
    try:
        # feedparser ä¼šè‡ªåŠ¨æŠŠå„ç§æ—¶é—´æ ¼å¼è§£ææˆ struct_time
        published_struct = getattr(entry, 'published_parsed', None) or getattr(entry, 'updated_parsed', None)
        
        if not published_struct:
            # å¦‚æœå®åœ¨æ‰¾ä¸åˆ°æ—¶é—´ï¼Œä¸ºäº†ä¿é™©èµ·è§ï¼Œå‡è®¾å®ƒæ˜¯æ–°çš„ï¼ˆæˆ–è€…ä½ å¯ä»¥æ”¹ä¸º False ä¸¢å¼ƒï¼‰
            return True
            
        # è½¬æ¢ä¸º datetime å¯¹è±¡
        pub_date = datetime.datetime.fromtimestamp(mktime(published_struct))
        current_date = datetime.datetime.now()
        
        # è®¡ç®—æ—¶é—´å·®
        delta = current_date - pub_date
        
        if delta.days <= MAX_LOOKBACK_DAYS:
            return True
        else:
            return False
    except Exception as e:
        print(f"æ—¶é—´è§£æé”™è¯¯: {e}")
        return True # å‡ºé”™æ—¶é»˜è®¤ä¿ç•™

def fetch_rss_papers():
    print(f"å¼€å§‹æŠ“å–ä»»åŠ¡... (åªçœ‹æœ€è¿‘ {MAX_LOOKBACK_DAYS} å¤©)")
    found_papers = []
    
    for source in RSS_FEEDS:
        print(f"æ­£åœ¨æ£€æŸ¥: {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                continue

            for entry in feed.entries:
                # --- [æ–°å¢] æ—¶é—´è¿‡æ»¤å™¨ ---
                if not is_recent_paper(entry):
                    continue # å¦‚æœå¤ªæ—§ï¼Œç›´æ¥è·³è¿‡ï¼Œçœ‹ä¸‹ä¸€ç¯‡
                # -----------------------

                title = entry.title
                summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                # è·å–å±•ç¤ºç”¨çš„æ—¶é—´å­—ç¬¦ä¸²
                published = getattr(entry, 'published', '') or getattr(entry, 'updated', 'Unknown Date')
                
                content_to_check = (title + summary).lower()
                
                matched_keywords = []
                for kw in KEYWORD_LIST:
                    if kw.lower() in content_to_check:
                        matched_keywords.append(kw)
                
                if matched_keywords:
                    paper_info = {
                        'source': source['name'],
                        'title': title.replace('\n', ' '),
                        'link': link,
                        'date': published,
                        'keywords': matched_keywords
                    }
                    found_papers.append(paper_info)
                    
        except Exception as e:
            print(f"æŠ“å– {source['name']} å¤±è´¥: {e}")
            
    return found_papers

def generate_markdown(papers):
    if not papers:
        return None
    
    date_str = get_current_date()
    md_content = f"# ğŸ“… Daily Paper Update: {date_str}\n\n"
    md_content += f"**ä»Šæ—¥å‘ç° {len(papers)} ç¯‡è¿‘æœŸ({MAX_LOOKBACK_DAYS}å¤©å†…)ç›¸å…³è®ºæ–‡**\n\n---"
    
    current_source = ""
    for paper in papers:
        if paper['source'] != current_source:
            current_source = paper['source']
            md_content += f"\n\n## ğŸ“š {current_source}\n"
        
        kw_str = ", ".join([f"`{k}`" for k in paper['keywords']])
        md_content += f"\n### [{paper['title']}]({paper['link']})\n"
        md_content += f"- **å…³é”®è¯**: {kw_str}\n"
        md_content += f"- **å‘å¸ƒæ—¶é—´**: {paper['date']}\n"
        
    return md_content

def post_github_issue(content):
    if not content:
        print("ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„æ–°è®ºæ–‡ã€‚")
        return

    if not TOKEN:
        print("é”™è¯¯ï¼šæœªè®¾ç½® GH_TOKENï¼Œæ— æ³•å‘é€ Issueã€‚")
        return

    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    date_str = get_current_date()
    payload = {
        "title": f"[{date_str}] Daily Papers ({len(content.split('###')) - 1} papers)",
        "body": content,
        "labels": ["daily-report"]
    }
    
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    
    if response.status_code == 201:
        print("âœ… Issue åˆ›å»ºæˆåŠŸï¼")
    else:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
        print(response.text)

if __name__ == '__main__':
    papers = fetch_rss_papers()
    md_text = generate_markdown(papers)
    post_github_issue(md_text)
