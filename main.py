# encoding: utf-8
import feedparser
import requests
import json
import datetime
import time
import pytz
from time import mktime
from config import *

# === è®¾ç½®å›é¡¾æ—¶é—´èŒƒå›´ ===
MAX_LOOKBACK_DAYS = 30 

# === æ–°å¢ï¼šå»é‡æ£€æµ‹èŒƒå›´ ===
# æ£€æŸ¥è¿‡å»å¤šå°‘ä¸ª Issue æ¥é˜²æ­¢é‡å¤ï¼Ÿå»ºè®®è®¾ä¸º 30 (è¦†ç›–è¿‡å»ä¸€ä¸ªæœˆ)
DUPLICATE_CHECK_COUNT = 30

def get_current_date():
    tz = pytz.timezone('Asia/Shanghai')
    return datetime.datetime.now(tz).strftime('%Y-%m-%d')

def is_recent_paper(entry):
    """åˆ¤æ–­è®ºæ–‡æ˜¯å¦åœ¨æœ€è¿‘ MAX_LOOKBACK_DAYS å¤©å†…å‘å¸ƒ"""
    try:
        published_struct = getattr(entry, 'published_parsed', None) or getattr(entry, 'updated_parsed', None)
        if not published_struct:
            return True
        pub_date = datetime.datetime.fromtimestamp(mktime(published_struct))
        current_date = datetime.datetime.now()
        delta = current_date - pub_date
        return delta.days <= MAX_LOOKBACK_DAYS
    except Exception as e:
        return True 

def get_already_sent_links():
    """
    è·å–æœ€è¿‘å‘å¸ƒçš„ Issue å†…å®¹ï¼Œæå–å‡ºæ‰€æœ‰å·²å‘é€è¿‡çš„é“¾æ¥ã€‚
    ç”¨äºå»é‡ã€‚
    """
    if not TOKEN:
        print("è­¦å‘Šï¼šæœªè®¾ç½® Tokenï¼Œæ— æ³•è·å–å†å²è®°å½•è¿›è¡Œå»é‡ã€‚")
        return set()

    print(f"æ­£åœ¨æ£€æŸ¥å†å² Issue ä»¥å»é‡ (æ£€æŸ¥æœ€è¿‘ {DUPLICATE_CHECK_COUNT} ä¸ª)...")
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    params = {
        "state": "all",          # å³ä½¿å…³é—­çš„ Issue ä¹Ÿè¦æ£€æŸ¥
        "labels": "daily-report", # åªæ£€æŸ¥æˆ‘ä»¬æœºå™¨äººå‘çš„
        "per_page": DUPLICATE_CHECK_COUNT
    }
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code != 200:
            print(f"è·å–å†å²è®°å½•å¤±è´¥: {response.status_code}")
            return set()
        
        issues = response.json()
        sent_links = set()
        
        # éå†å†å² Issue çš„å†…å®¹
        for issue in issues:
            body = issue.get('body', '')
            if body:
                # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å¤„ç†ï¼šç›´æ¥æŠŠ body å½“ä½œå¤§å­—ç¬¦ä¸²
                # å¦‚æœæ–°è®ºæ–‡çš„ link å­˜åœ¨äºè¿™ä¸ªå­—ç¬¦ä¸²ä¸­ï¼Œå°±è®¤ä¸ºå‘è¿‡äº†
                # ä¸ºäº†æé«˜å‡†ç¡®ç‡ï¼Œæˆ‘ä»¬å¯ä»¥åªæå– markdown é“¾æ¥ï¼Œä½†ç›´æ¥åŒ…å«æ£€æŸ¥é€šå¸¸å¤Ÿç”¨äº†
                sent_links.add(body) 
        
        print(f"âœ… å·²åŠ è½½å†å²è®°å½•ï¼Œå‡†å¤‡è¿‡æ»¤...")
        return sent_links # è¿”å›åŒ…å«æ‰€æœ‰å†å²æ–‡æœ¬çš„é›†åˆï¼ˆæˆ–é•¿å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        
    except Exception as e:
        print(f"è·å–å†å²è®°å½•å‡ºé”™: {e}")
        return set()

def fetch_rss_papers():
    # 1. å…ˆè·å–å†å²è®°å½•
    history_contents = get_already_sent_links()
    
    print(f"å¼€å§‹æŠ“å–ä»»åŠ¡... (åªçœ‹æœ€è¿‘ {MAX_LOOKBACK_DAYS} å¤©)")
    found_papers = []
    
    for source in RSS_FEEDS:
        print(f"æ­£åœ¨æ£€æŸ¥: {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                continue

            for entry in feed.entries:
                # --- æ—¶é—´è¿‡æ»¤å™¨ ---
                if not is_recent_paper(entry):
                    continue 

                title = entry.title
                summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                published = getattr(entry, 'published', '') or getattr(entry, 'updated', 'Unknown Date')
                
                # --- [æ–°å¢] æ ¸å¿ƒå»é‡é€»è¾‘ ---
                # æ£€æŸ¥è¿™ä¸ªé“¾æ¥æ˜¯å¦åœ¨ä¹‹å‰çš„ Issue æ­£æ–‡é‡Œå‡ºç°è¿‡
                is_duplicate = False
                for old_body in history_contents:
                    if link in old_body:
                        is_duplicate = True
                        break
                
                if is_duplicate:
                    # print(f"è·³è¿‡å·²å‘é€è®ºæ–‡: {title}") # è°ƒè¯•æ—¶å¯æ‰“å¼€
                    continue
                # -----------------------

                content_to_check = (title + summary).lower()
                
                matched_keywords = []
                for kw in KEYWORD_LIST:
                    if kw.lower() in content_to_check:
                        matched_keywords.append(kw)
                
                if matched_keywords:
                    paper_info = {
                        'source': source['name'],
                        'title': title.replace('\n', ' '),
                        'link': link,
                        'date': published,
                        'keywords': matched_keywords
                    }
                    found_papers.append(paper_info)
                    
        except Exception as e:
            print(f"æŠ“å– {source['name']} å¤±è´¥: {e}")
            
    return found_papers

def generate_markdown(papers):
    if not papers:
        return None
    
    date_str = get_current_date()
    md_content = f"# ğŸ“… Daily Paper Update: {date_str}\n\n"
    md_content += f"**ä»Šæ—¥å‘ç° {len(papers)} ç¯‡æ–°è®ºæ–‡**\n\n---"
    
    current_source = ""
    for paper in papers:
        if paper['source'] != current_source:
            current_source = paper['source']
            md_content += f"\n\n## ğŸ“š {current_source}\n"
        
        kw_str = ", ".join([f"`{k}`" for k in paper['keywords']])
        md_content += f"\n### [{paper['title']}]({paper['link']})\n"
        md_content += f"- **å…³é”®è¯**: {kw_str}\n"
        md_content += f"- **å‘å¸ƒæ—¶é—´**: {paper['date']}\n"
        
    return md_content

def post_github_issue(content):
    if not content:
        print("ä»Šæ—¥æ— æ–°å‘ç°ï¼ˆæˆ–å…¨éƒ¨å·²å»é‡ï¼‰ï¼Œä¸åˆ›å»º Issueã€‚")
        return

    if not TOKEN:
        print("é”™è¯¯ï¼šæœªè®¾ç½® GH_TOKENï¼Œæ— æ³•å‘é€ Issueã€‚")
        return

    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    date_str = get_current_date()
    payload = {
        "title": f"[{date_str}] Daily Papers ({len(content.split('###')) - 1} papers)",
        "body": content,
        "labels": ["daily-report"]
    }
    
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    
    if response.status_code == 201:
        print("âœ… Issue åˆ›å»ºæˆåŠŸï¼")
    else:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
        print(response.text)

if __name__ == '__main__':
    papers = fetch_rss_papers()
    md_text = generate_markdown(papers)
    post_github_issue(md_text)# encoding: utf-8
import feedparser
import requests
import json
import datetime
import time
import pytz
from time import mktime
from config import *

# === è®¾ç½®å›é¡¾æ—¶é—´èŒƒå›´ ===
# åªæŠ“å–è¿‡å» 14 å¤©å†…çš„è®ºæ–‡ï¼Œé¿å…æŠ“åˆ°è€æ—§æ•°æ®
MAX_LOOKBACK_DAYS = 14 

def get_current_date():
    tz = pytz.timezone('Asia/Shanghai')
    return datetime.datetime.now(tz).strftime('%Y-%m-%d')

def is_recent_paper(entry):
    """
    åˆ¤æ–­è®ºæ–‡æ˜¯å¦åœ¨æœ€è¿‘ MAX_LOOKBACK_DAYS å¤©å†…å‘å¸ƒ
    """
    try:
        # feedparser ä¼šè‡ªåŠ¨æŠŠå„ç§æ—¶é—´æ ¼å¼è§£ææˆ struct_time
        published_struct = getattr(entry, 'published_parsed', None) or getattr(entry, 'updated_parsed', None)
        
        if not published_struct:
            # å¦‚æœå®åœ¨æ‰¾ä¸åˆ°æ—¶é—´ï¼Œä¸ºäº†ä¿é™©èµ·è§ï¼Œå‡è®¾å®ƒæ˜¯æ–°çš„ï¼ˆæˆ–è€…ä½ å¯ä»¥æ”¹ä¸º False ä¸¢å¼ƒï¼‰
            return True
            
        # è½¬æ¢ä¸º datetime å¯¹è±¡
        pub_date = datetime.datetime.fromtimestamp(mktime(published_struct))
        current_date = datetime.datetime.now()
        
        # è®¡ç®—æ—¶é—´å·®
        delta = current_date - pub_date
        
        if delta.days <= MAX_LOOKBACK_DAYS:
            return True
        else:
            return False
    except Exception as e:
        print(f"æ—¶é—´è§£æé”™è¯¯: {e}")
        return True # å‡ºé”™æ—¶é»˜è®¤ä¿ç•™

def fetch_rss_papers():
    print(f"å¼€å§‹æŠ“å–ä»»åŠ¡... (åªçœ‹æœ€è¿‘ {MAX_LOOKBACK_DAYS} å¤©)")
    found_papers = []
    
    for source in RSS_FEEDS:
        print(f"æ­£åœ¨æ£€æŸ¥: {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                continue

            for entry in feed.entries:
                # --- [æ–°å¢] æ—¶é—´è¿‡æ»¤å™¨ ---
                if not is_recent_paper(entry):
                    continue # å¦‚æœå¤ªæ—§ï¼Œç›´æ¥è·³è¿‡ï¼Œçœ‹ä¸‹ä¸€ç¯‡
                # -----------------------

                title = entry.title
                summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                # è·å–å±•ç¤ºç”¨çš„æ—¶é—´å­—ç¬¦ä¸²
                published = getattr(entry, 'published', '') or getattr(entry, 'updated', 'Unknown Date')
                
                content_to_check = (title + summary).lower()
                
                matched_keywords = []
                for kw in KEYWORD_LIST:
                    if kw.lower() in content_to_check:
                        matched_keywords.append(kw)
                
                if matched_keywords:
                    paper_info = {
                        'source': source['name'],
                        'title': title.replace('\n', ' '),
                        'link': link,
                        'date': published,
                        'keywords': matched_keywords
                    }
                    found_papers.append(paper_info)
                    
        except Exception as e:
            print(f"æŠ“å– {source['name']} å¤±è´¥: {e}")
            
    return found_papers

def generate_markdown(papers):
    if not papers:
        return None
    
    date_str = get_current_date()
    md_content = f"# ğŸ“… Daily Paper Update: {date_str}\n\n"
    md_content += f"**ä»Šæ—¥å‘ç° {len(papers)} ç¯‡è¿‘æœŸ({MAX_LOOKBACK_DAYS}å¤©å†…)ç›¸å…³è®ºæ–‡**\n\n---"
    
    current_source = ""
    for paper in papers:
        if paper['source'] != current_source:
            current_source = paper['source']
            md_content += f"\n\n## ğŸ“š {current_source}\n"
        
        kw_str = ", ".join([f"`{k}`" for k in paper['keywords']])
        md_content += f"\n### [{paper['title']}]({paper['link']})\n"
        md_content += f"- **å…³é”®è¯**: {kw_str}\n"
        md_content += f"- **å‘å¸ƒæ—¶é—´**: {paper['date']}\n"
        
    return md_content

def post_github_issue(content):
    if not content:
        print("ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„æ–°è®ºæ–‡ã€‚")
        return

    if not TOKEN:
        print("é”™è¯¯ï¼šæœªè®¾ç½® GH_TOKENï¼Œæ— æ³•å‘é€ Issueã€‚")
        return

    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    date_str = get_current_date()
    payload = {
        "title": f"[{date_str}] Daily Papers ({len(content.split('###')) - 1} papers)",
        "body": content,
        "labels": ["daily-report"]
    }
    
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    
    if response.status_code == 201:
        print("âœ… Issue åˆ›å»ºæˆåŠŸï¼")
    else:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
        print(response.text)

if __name__ == '__main__':
    papers = fetch_rss_papers()
    md_text = generate_markdown(papers)
    post_github_issue(md_text)
