# encoding: utf-8
import feedparser
import requests
import json
import datetime
import time
import pytz
from config import *

def get_current_date():
    # è·å–åŒ—äº¬æ—¶é—´
    tz = pytz.timezone('Asia/Shanghai')
    return datetime.datetime.now(tz).strftime('%Y-%m-%d')

def fetch_rss_papers():
    print(f"å¼€å§‹æŠ“å–ä»»åŠ¡... å…± {len(RSS_FEEDS)} ä¸ªè®¢é˜…æº")
    found_papers = []
    
    for source in RSS_FEEDS:
        print(f"æ­£åœ¨æ£€æŸ¥: {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
            if not feed.entries:
                continue

            for entry in feed.entries:
                # è·å–æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆä¸åŒRSSæºå­—æ®µåå¯èƒ½ä¸åŒï¼Œåšä¸ªå®¹é”™ï¼‰
                title = entry.title
                summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                published = getattr(entry, 'published', '') or getattr(entry, 'updated', 'Unknown Date')
                
                # å…³é”®è¯åŒ¹é… (æ ‡é¢˜ æˆ– æ‘˜è¦ åŒ…å«å…³é”®è¯)
                # å°†æ ‡é¢˜å’Œæ‘˜è¦è½¬ä¸ºå°å†™è¿›è¡Œæ¯”å¯¹
                content_to_check = (title + summary).lower()
                
                matched_keywords = []
                for kw in KEYWORD_LIST:
                    if kw.lower() in content_to_check:
                        matched_keywords.append(kw)
                
                if matched_keywords:
                    # æ‰¾åˆ°ç¬¦åˆçš„è®ºæ–‡ï¼
                    paper_info = {
                        'source': source['name'],
                        'title': title.replace('\n', ' '),
                        'link': link,
                        'date': published,
                        'keywords': matched_keywords
                    }
                    found_papers.append(paper_info)
                    
        except Exception as e:
            print(f"æŠ“å– {source['name']} å¤±è´¥: {e}")
            
    return found_papers

def generate_markdown(papers):
    if not papers:
        return None
    
    date_str = get_current_date()
    md_content = f"# ğŸ“… Daily Paper Update: {date_str}\n\n"
    md_content += f"**ä»Šæ—¥å‘ç° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡**\n\n---"
    
    # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤º
    current_source = ""
    for paper in papers:
        if paper['source'] != current_source:
            current_source = paper['source']
            md_content += f"\n\n## ğŸ“š {current_source}\n"
        
        # æ ¼å¼åŒ–æ¯ç¯‡è®ºæ–‡
        kw_str = ", ".join([f"`{k}`" for k in paper['keywords']])
        md_content += f"\n### [{paper['title']}]({paper['link']})\n"
        md_content += f"- **å…³é”®è¯**: {kw_str}\n"
        md_content += f"- **å‘å¸ƒæ—¶é—´**: {paper['date']}\n"
        
    return md_content

def post_github_issue(content):
    if not content:
        print("ä»Šæ—¥æ— æ–°å‘ç°ï¼Œä¸åˆ›å»º Issueã€‚")
        return

    if not TOKEN:
        print("é”™è¯¯ï¼šæœªè®¾ç½® GH_TOKENï¼Œæ— æ³•å‘é€ Issueã€‚")
        return

    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    headers = {
        "Authorization": f"token {TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    date_str = get_current_date()
    payload = {
        "title": f"[{date_str}] Daily Papers ({len(content.split('###')) - 1} papers)",
        "body": content,
        "labels": ["daily-report"]
    }
    
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    
    if response.status_code == 201:
        print("âœ… Issue åˆ›å»ºæˆåŠŸï¼")
    else:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
        print(response.text)

if __name__ == '__main__':
    # 1. æŠ“å–
    papers = fetch_rss_papers()
    
    # 2. ç”Ÿæˆå†…å®¹
    md_text = generate_markdown(papers)
    
    # 3. å‘é€åˆ° GitHub Issue
    post_github_issue(md_text)
